{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The German credit dataset is a standard imbalanced classification dataset that has this property of differing costs to misclassification errors. Models evaluated on this dataset can be evaluated using the Fbeta-Measure that provides a way of both quantifying model performance generally, and captures the requirement that one type of misclassification error is more costly than another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was used as part of the Statlog project, a European-based initiative in the 1990s to evaluate and compare a large number (at the time) of machine learning algorithms on a range of different classification tasks. The dataset is credited to Hans Hofmann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset includes 1,000 examples and 20 input variables, 7 of which are numerical (integer) and 13 are categorical.\n",
    "\n",
    "* Status of existing checking account\n",
    "* Duration in month\n",
    "* Credit history\n",
    "* Purpose\n",
    "* Credit amount\n",
    "* Savings account\n",
    "* Present employment since\n",
    "* Installment rate in percentage of disposable income\n",
    "* Personal status and sex\n",
    "* Other debtors\n",
    "* Present residence since\n",
    "* Property\n",
    "* Age in years\n",
    "* Other installment plans\n",
    "* Housing\n",
    "* Number of existing credits at this bank\n",
    "* Job\n",
    "* Number of dependents\n",
    "* Telephone\n",
    "* Foreign worker\n",
    "\n",
    "Some of the categorical variables have an ordinal relationship, such as “Savings account,” although most do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 21)\n",
      "Class=1, Count=700, Percentage=70.000%\n",
      "Class=2, Count=300, Percentage=30.000%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "# define the dataset location\n",
    "filename = 'german.csv'\n",
    "# load the csv file as a data frame\n",
    "df = pd.read_csv(filename, header=None)\n",
    "# summarize the shape of the dataset\n",
    "print(df.shape)\n",
    "# summarize the class distribution\n",
    "target = df.values[:,-1]\n",
    "counter = Counter(target)\n",
    "for k,v in counter.items():\n",
    "    per = v / len(target) * 100\n",
    "    print('Class=%d, Count=%d, Percentage=%.3f%%' % (k, v, per))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first loads the dataset and confirms the number of rows and columns, that is 1,000 rows and 20 input variables and 1 target variable.\n",
    "\n",
    "The class distribution is then summarized, confirming the number of good and bad customers and the percentage of cases in the minority and majority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# select columns with numerical data types\n",
    "num_ix = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "# select a subset of the dataframe with the chosen columns\n",
    "subset = df[num_ix]\n",
    "# create a histogram plot of each numeric variable\n",
    "ax = subset.hist()\n",
    "# disable axis labels to avoid the clutter\n",
    "for axis in ax.flatten():\n",
    "    axis.set_xticklabels([])\n",
    "    axis.set_yticklabels([])\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see many different distributions, some with Gaussian-like distributions, others with seemingly exponential or discrete distributions.\n",
    "\n",
    "Depending on the choice of modeling algorithms, we would expect scaling the distributions to the same range to be useful, and perhaps the use of some power transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Test and Baseline Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate candidate models using repeated stratified k-fold cross-validation.\n",
    "\n",
    "The k-fold cross-validation procedure provides a good general estimate of model performance that is not too optimistically biased, at least compared to a single train-test split. We will use k=10, meaning each fold will contain about 1000/10 or 100 examples.\n",
    "\n",
    "Stratified means that each fold will contain the same mixture of examples by class, that is about 70 percent to 30 percent good to bad customers. Repeated means that the evaluation process will be performed multiple times to help avoid fluke results and better capture the variance of the chosen model. We will use three repeats.\n",
    "\n",
    "This means a single model will be fit and evaluated 10 * 3 or 30 times and the mean and standard deviation of these runs will be reported.\n",
    "\n",
    "This can be achieved using the RepeatedStratifiedKFold scikit-learn class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be achieved by using a version of the F-measure that calculates a weighted harmonic mean of precision and recall but favors higher recall scores over precision scores. This is called the Fbeta-measure, a generalization of F-measure, where “beta” is a parameter that defines the weighting of the two scores.\n",
    "\n",
    "Fbeta-Measure = ((1 + beta^2) * Precision * Recall) / (beta^2 * Precision + Recall)\n",
    "A beta value of 2 will weight more attention on recall than precision and is referred to as the F2-measure.\n",
    "\n",
    "F2-Measure = ((1 + 2^2) * Precision * Recall) / (2^2 * Precision + Recall)\n",
    "We will use this measure to evaluate models on the German credit dataset. This can be achieved using the fbeta_score() scikit-learn function.\n",
    "\n",
    "We can define a function to load the dataset and split the columns into input and output variables. We will one-hot encode the categorical variables and label encode the target variable. You might recall that a one-hot encoding replaces the categorical variable with one new column for each value of the variable and marks values with a 1 in the column for that value.\n",
    "\n",
    "First, we must split the DataFrame into input and output variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The load_dataset() function below ties all of this together and loads and prepares the dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "    # load the dataset as a numpy array\n",
    "    dataframe = read_csv(full_path, header=None)\n",
    "    # split into inputs and outputs\n",
    "    last_ix = len(dataframe.columns) - 1\n",
    "    X, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "    # select categorical features\n",
    "    cat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "    # one hot encode cat features only\n",
    "    ct = ColumnTransformer([('o',OneHotEncoder(),cat_ix)], remainder='passthrough')\n",
    "    X = ct.fit_transform(X)\n",
    "    # label encode the target variable to have the classes 0 and 1\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a function that will evaluate a set of predictions using the fbeta_score() function with beta set to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X, y, model):\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # define the model evaluation the metric\n",
    "    metric = make_scorer(f2)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 61) (1000,) Counter({0: 700, 1: 300})\n",
      "Mean F2: 0.682 (0.000)\n"
     ]
    }
   ],
   "source": [
    "# define the location of the dataset\n",
    "full_path = 'german.csv'\n",
    "# load the dataset\n",
    "X, y = load_dataset(full_path)\n",
    "# summarize the loaded dataset\n",
    "print(X.shape, y.shape, Counter(y))\n",
    "# define the reference model\n",
    "model = DummyClassifier(strategy='constant', constant=1)\n",
    "# evaluate the model\n",
    "scores = evaluate_model(X, y, model)\n",
    "# summarize performance\n",
    "print('Mean F2: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that the baseline algorithm achieves an F2-Measure of about 0.682. This score provides a lower limit on model skill; any model that achieves an average F2-Measure above about 0.682 has skill, whereas models that achieve a score below this value do not have skill on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">LR 0.487 (0.103)\n",
      ">LDA 0.505 (0.093)\n",
      ">NB 0.632 (0.062)\n",
      ">GPC 0.219 (0.081)\n",
      ">SVM 0.436 (0.103)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.svm import SVC\n",
    " \n",
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "    # load the dataset as a numpy array\n",
    "    dataframe = read_csv(full_path, header=None)\n",
    "    # split into inputs and outputs\n",
    "    last_ix = len(dataframe.columns) - 1\n",
    "    X, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "    # select categorical and numerical features\n",
    "    cat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "    num_ix = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    # label encode the target variable to have the classes 0 and 1\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return X.values, y, cat_ix, num_ix\n",
    " \n",
    "# calculate f2-measure\n",
    "def f2_measure(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=2)\n",
    " \n",
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # define the model evaluation metric\n",
    "    metric = make_scorer(f2_measure)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    " \n",
    "# define models to test\n",
    "def get_models():\n",
    "    models, names = list(), list()\n",
    "    # LR\n",
    "    models.append(LogisticRegression(solver='liblinear'))\n",
    "    names.append('LR')\n",
    "    # LDA\n",
    "    models.append(LinearDiscriminantAnalysis())\n",
    "    names.append('LDA')\n",
    "    # NB\n",
    "    models.append(GaussianNB())\n",
    "    names.append('NB')\n",
    "    # GPC\n",
    "    models.append(GaussianProcessClassifier())\n",
    "    names.append('GPC')\n",
    "    # SVM\n",
    "    models.append(SVC(gamma='scale'))\n",
    "    names.append('SVM')\n",
    "    return models, names\n",
    " \n",
    "# define the location of the dataset\n",
    "full_path = 'german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)\n",
    "# define models\n",
    "models, names = get_models()\n",
    "results = list()\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "    # one hot encode categorical, normalize numerical\n",
    "    ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "    # wrap the model i a pipeline\n",
    "    pipeline = Pipeline(steps=[('t',ct),('m',models[i])])\n",
    "    # evaluate the model and store results\n",
    "    scores = evaluate_model(X, y, pipeline)\n",
    "    results.append(scores)\n",
    "    # summarize and store\n",
    "    print('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))\n",
    "# plot the results\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that none of the tested models have an F2-measure above the default of predicting the majority class in all cases (0.682). None of the models are skillful. This is surprising, although suggests that perhaps the decision boundary between the two classes is noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some results, let’s see if we can improve them with some undersampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling is perhaps the least widely used technique when addressing an imbalanced classification task as most of the focus is put on oversampling the majority class with SMOTE.\n",
    "\n",
    "Undersampling can help to remove examples from the majority class along the decision boundary that make the problem challenging for classification algorithms.\n",
    "\n",
    "In this experiment we will test the following undersampling algorithms:\n",
    "\n",
    "* Tomek Links (TL)\n",
    "* Edited Nearest Neighbors (ENN)\n",
    "* Repeated Edited Nearest Neighbors (RENN)\n",
    "* One Sided Selection (OSS)\n",
    "* Neighborhood Cleaning Rule (NCR)\n",
    "\n",
    "The Tomek Links and ENN methods select examples from the majority class to delete, whereas OSS and NCR both select examples to keep and examples to delete. We will use the balanced version of the logistic regression algorithm to test each undersampling method, to keep things simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pipeline provided by scikit-learn does not know about undersampling algorithms. Therefore, we must use the Pipeline implementation provided by the imbalanced-learn library.\n",
    "\n",
    "As in the previous section, the first step of the pipeline will be one hot encoding of categorical variables and normalization of numerical variables, and the final step will be fitting the model. Here, the middle step will be the undersampling technique, correctly applied within the cross-validation evaluation on the training dataset only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the example evaluates the logistic regression algorithm with five different undersampling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your specific results will vary given the stochastic nature of the learning algorithms; consider running the example a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">TL 0.670 (0.062)\n",
      ">ENN 0.697 (0.052)\n",
      ">RENN 0.712 (0.038)\n",
      ">OSS 0.668 (0.061)\n",
      ">NCR 0.682 (0.050)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAFENJREFUeJzt3X+QXWd93/H3BxHbEAaQInmaWhYSRGYIaWond5y0nrZQYiOYBNPQpnI6DZ7S+I9imLqFxm6Y4hrcMCQMoTNqjEldu2mJ8TjUFhmKMXXdoRl7qlWwTaSMQRYlFpqGta2Q8djY+vHtH/fKOr7sau9q7+7eu8/7NXNHe855zt3vOdr93Gefe85zU1VIktrwktUuQJK0cgx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkNeutoFDNu4cWNt3bp1tcuQpKmyd+/eJ6pq00LtJi70t27dyszMzGqXIUlTJcm3R2nn8I4kNcTQl6SGGPqS1BBDX5IaYuhLUkNGCv0kO5I8muRAkmvn2L4lyf9M8rUkjyR5e2fbdYP9Hk3y1nEWL0lanAUv2UyyDtgFXAocAvYk2V1V+zvNPgTcUVW/k+THgS8CWwdf7wTeCPxV4CtJLqiq4+M+EEnSwkbp6V8MHKiqg1X1PHA7cPlQmwJeOfj6VcDhwdeXA7dX1XNV9S3gwOD5JEmrYJSbs84DHu8sHwJ+ZqjN9cCXk7wP+GHg5zr7Pji073lnVKmWTZIlP4eftSxNh1F6+nMlwvBv+BXArVW1GXg78HtJXjLiviS5KslMkpnZ2dkRStI4VdVpH6O2kTT5Rgn9Q8D5neXNnBq+Oek9wB0AVfUAcA6wccR9qaqbq6pXVb1NmxacOkKSdIZGCf09wPYk25KcRf+N2d1Dbf4MeAtAkjfQD/3ZQbudSc5Osg3YDvyfcRUvSVqcBcf0q+pYkquBe4B1wC1VtS/JDcBMVe0G/iXwmSTX0B++ubL6f/PvS3IHsB84BrzXK3ckafVk0sZje71eOcvmZEniuL004ZLsrareQu28I1eSGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQ0aZT1+SmjWOz5uAyfnMCUNfkk5jlLCepvmpHN6RpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0JakhzV6nP44bLqblulxJOqnZ0F8osKfpZgtJGpXDO5LUkGZ7+tKwtTbHijQXQ18aWGtzrEhzcXhHkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNWSk0E+yI8mjSQ4kuXaO7Z9M8tDg8Y0kf9HZdryzbfc4i5ckLc6C1+knWQfsAi4FDgF7kuyuqv0n21TVNZ327wMu6jzFs1V14fhKliSdqVF6+hcDB6rqYFU9D9wOXH6a9lcAvz+O4iStjiRjeWjyjBL65wGPd5YPDdb9gCSvAbYB93VWn5NkJsmDSd55xpVKWjFVteBjlHaaPKNMwzDXy/V8/5s7gTur6nhn3ZaqOpzktcB9Sb5eVY+96BskVwFXAWzZsmWEkiRJZ2KUnv4h4PzO8mbg8DxtdzI0tFNVhwf/HgTu58Xj/Sfb3FxVvarqbdq0aYSSpMXbsGHDWIYrlvocGzZsWOUzoZaN0tPfA2xPsg34Dv1g/+XhRkleD6wHHuisWw88U1XPJdkIXAJ8fByFS4t15MiRiRhycKxbq2nB0K+qY0muBu4B1gG3VNW+JDcAM1V18jLMK4Db68W/VW8APp3kBP2/Kj7WvepHkrSyMgk9n65er1czMzOrXYZT6HaslXMxKccxKXUs1Vo5jnGYhHORZG9V9RZq5x25ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGG/hrnXaiSuka5I1dTzLtQJXXZ05ekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfapBzMrXLuXekBjknU7vs6UtSQwx9aUSzz8xy5Zeu5Ilnn1jtUqQzZuhLI7rpkZv44z//Y256+KbVLkVj1Nr7G4a+NILZZ2a5+8DdFMVdB+6yt7+GnHx/Y7UfR44cWZHjNfSlEdz0yE2cqBMAnKgT9vY1tQx9aQEne/lHTxwF4OiJo/b2NbXWZOi3Nkan5dXt5Z9kb1/Tak1ep+81yKfUh18J179qtcvo1zGlHv7uwy/08k86euIoD333oVWqSDpzmYRw7Or1ejUzM7Ok50gyMaG/2nVMQg2TUsck1DApdUxCDZNSxyTUMI46kuytqt5C7dbk8I4kaW4jDe8k2QF8ClgH/G5VfWxo+yeBNw8WXw6cW1WvHmx7N/ChwbaPVtVt4yhcWiyHuqQRQj/JOmAXcClwCNiTZHdV7T/Zpqqu6bR/H3DR4OsNwIeBHlDA3sG+K3NBqtSRf/uXk/Nn/PWrXYVaNcrwzsXAgao6WFXPA7cDl5+m/RXA7w++fitwb1U9NQj6e4EdSylYK8upB6S1ZZTQPw94vLN8aLDuByR5DbANuG8x+ya5KslMkpnZ2dlR6tYKceoBaW0ZJfTnuu5wvr+RdwJ3VtXxxexbVTdXVa+qeps2bRqhJK0Epx6Q1p5RQv8QcH5neTNweJ62Ozk1tLPYfTVhnHpAWntGCf09wPYk25KcRT/Ydw83SvJ6YD3wQGf1PcBlSdYnWQ9cNlinCefUA9LatGDoV9Ux4Gr6Yf2nwB1VtS/JDUne0Wl6BXB7dS6PqKqngI/Qf+HYA9wwWKcJ59QD0to00nX6VfVF4ItD6/7N0PL18+x7C3DLGdanVeLUA9LatCbn3tHS3fmOO1e7BEnLwGkYJKkhhr4kNcTQl6SGGPqS1BBDX5KWYNrmpzL0JWkJpm1+KkNfks7QNM5PtSY/LnESPijjBdd/b1W//Vr5KLi1UsPE1OHvSOf7n/m5+MiPrOe/veIVHH1J+KETxS8+/TQfenIJHxeyhHMx6sclrsnQn4hfqgmpYxJqmJQ6JqGGSaljEmqYlDrOtIbZZ2Z52+ffxnPHn3th3dnrzuZL7/oSG1+2ccXq6OzvZ+RKw5Ks+mP9+vWrfRo0BtM6P5XTMKgZ4+hRTkLPVJNhWuenMvQl6QxM6/xUDu9IUkMMfUlqiKE/h2m7w06SRmXoz2Ha7rCTVpodo+ll6A+ZxjvspJVmx2h6GfpDutfeTsM1t9JKs2M03Qz9jpM/zCevvT164qg/1NIQO0bTzdDvmNY77KSVYsdo+hn6HdN6h520UuwYTT/vyO2Y1jvspJVix2j6GfqSRmbHaPoZ+g1IstolOLOkNCEM/TXOmSUldflGriQ1xNCXpIY4vCOpeS297zVSTz/JjiSPJjmQ5Np52vxSkv1J9iX5bGf98SQPDR67x1W4JI1DVS35MY7neeqpp1bkeBfs6SdZB+wCLgUOAXuS7K6q/Z0224HrgEuq6kiScztP8WxVXTjmuiVJZ2CUnv7FwIGqOlhVzwO3A5cPtflVYFdVHQGoqu+Ot0xJ0jiMEvrnAY93lg8N1nVdAFyQ5I+SPJhkR2fbOUlmBuvfucR6JUlLMMobuXO9wzF80fZLge3Am4DNwFeT/ERV/QWwpaoOJ3ktcF+Sr1fVYy/6BslVwFUAW7ZsWeQhSJJGNUpP/xBwfmd5M3B4jjZ3V9XRqvoW8Cj9FwGq6vDg34PA/cBFw9+gqm6uql5V9TZt2rTog5AkjWaU0N8DbE+yLclZwE5g+Cqcu4A3AyTZSH+452CS9UnO7qy/BNiPNIGSLPgYpZ00yRYc3qmqY0muBu4B1gG3VNW+JDcAM1W1e7DtsiT7gePAB6vqySR/E/h0khP0X2A+1r3qR5okTjWhFmTSftB7vV7NzMws6TkmZa6YSaljqdbKceiUSfk/nZQ6lmoSjiPJ3qrqLdTOaRgkqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1JBRPkRlKk3CFLcr9en2kjSqNRn645jtbhJmzZOkcXN4R5IaYuhLUkPW5PCOpIX5vlebDH2pQb7v1S6HdySpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUkJFCP8mOJI8mOZDk2nna/FKS/Un2JflsZ/27k3xz8Hj3uAqXJC3egrNsJlkH7AIuBQ4Be5Lsrqr9nTbbgeuAS6rqSJJzB+s3AB8GekABewf7Hhn/oUiSFjJKT/9i4EBVHayq54HbgcuH2vwqsOtkmFfVdwfr3wrcW1VPDbbdC+wYT+mSpMUaJfTPAx7vLB8arOu6ALggyR8leTDJjkXsS5KrkswkmZmdnR29eknSoowS+nN9vM7wJye8FNgOvAm4AvjdJK8ecV+q6uaq6lVVb9OmTSOUJEk6E6OE/iHg/M7yZuDwHG3urqqjVfUt4FH6LwKj7CtJWiGjhP4eYHuSbUnOAnYCu4fa3AW8GSDJRvrDPQeBe4DLkqxPsh64bLBOkrQKFrx6p6qOJbmaflivA26pqn1JbgBmqmo3p8J9P3Ac+GBVPQmQ5CP0XzgAbqiqp5bjQCRJC8ukfbBxr9ermZmZ1S6jqQ99TuZ662VxWjlXOqWl35GFTMK5SLK3qnoLtVuwp6+1b7V/WCWtHKdhkKSGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIV6nL0mnMerNiwu1m5T7YQx9STqNSQnrcXF4R5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWrISKGfZEeSR5McSHLtHNuvTDKb5KHB4592th3vrN89zuIlSYuz4AejJ1kH7AIuBQ4Be5Lsrqr9Q00/V1VXz/EUz1bVhUsvVZK0VKP09C8GDlTVwap6HrgduHx5y5IkLYdRQv884PHO8qHBumHvSvJIkjuTnN9Zf06SmSQPJnnnUoqVJC3NKKGfOdbV0PIXgK1V9ZPAV4DbOtu2VFUP+GXgt5O87ge+QXLV4IVhZnZ2dsTSJUmLNUroHwK6PffNwOFug6p6sqqeGyx+BvjpzrbDg38PAvcDFw1/g6q6uap6VdXbtGnTog5AkjS6UUJ/D7A9ybYkZwE7gRddhZPkRzuL7wD+dLB+fZKzB19vBC4Bht8AliStkAWv3qmqY0muBu4B1gG3VNW+JDcAM1W1G3h/kncAx4CngCsHu78B+HSSE/RfYD42x1U/kqQVkqrh4fnV1ev1amZmZtm/TzLXWxWLM2nnThqXcfx+gL8jKynJ3sH7p6e1YE9/rfKHUZqfvx9rl9MwSFJDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhoycXfkJpkFvr3adQAbgSdWu4gJ4bk4xXNxiufilEk4F6+pqgVnrJy40J8USWZGuaW5BZ6LUzwXp3guTpmmc+HwjiQ1xNCXpIYY+vO7ebULmCCei1M8F6d4Lk6ZmnPhmL4kNcSeviQ1pPnQT/IjSR4aPP5fku90lp9Z7fqWU5LjnWN9KMm1g/X3J5nptOsluX/w9ZuSVJJf6Gz/wyRvWun6l6pz/H+S5AtJXj1YvzXJs0Pn5lcG2/5vkj/oPMffT3Lr4Osrk5xI8pOd7X+SZOuKHtgYJNmc5O4k30zyWJJPJTkrycuT/NckXx8c2/9O8orBPr+eZF+SRwbn7GdW+ziWavCz/onO8geSXN9Z/pXBediXZH+SDwzW35rkW4Pz8HCSt6xC+XNq9kNUTqqqJ4ELAQb/mU9X1W8Nlp9exdJWwrNVdeE8285N8raq+u9zbDsE/DrwheUrbUW8cPxJbgPeC9w42PbYac5NL8kbq2rfHNtOnpt/OPZqV0j6H5v1eeB3quryJOvoj1nfSP/jUP+8qv7aoO3rgaNJ/gbw88BPVdVzg8/EPmt1jmCsngN+MclvVNWLrsNP8jbgnwOXVdXhJOcA/7jT5INVdWeSN9M/f9tXrOrTaL6nr3n9JvChebY9DHwvyaUrWM9yewA4b8S2vwX863m2/SHwxkEYTqu/C3y/qv4TQFUdB64B/gmwDfjOyYZV9WhVPQf8KPDE4Guq6omqOrzilY/fMfqBfc0c264DPnDyOKvq+1X1mTnaLeZna9kZ+m172dAQRrd3+gDw3KCXMpePMv+LwlQZ9GTfAuzurH7d0Ln5W51tdwA/leTH5ni6E8DHmf9FYRq8EdjbXVFVfwn8GfBfgF9L8kCSjyY52Xv9MnB+km8k+Q9J/s7KlrysdgH/KMmrhtb/BEPnaR47gLvGXtUZMvTb9mxVXdh5fG5o+7zBXlVfBRgKw2nzsiQPAU8CG4B7O9seGzo3X+1sO07/L6Hr5nnezwI/m2TbslS9/ALMdVlfgCPAa+kf/wZgT5I3VNXTwE8DVwGzwOeSXLky5S6vwQvefwbev8hdfzPJQfovlP9u7IWdIUNf86qq+4BzgJ+dp8mN9Mevp9XJMf3X0B9/fu8i9v094G8DW4Y3VNUx4BPAr42jyFWwD3jRlAJJXgmcT//F8Omq+nxV/TP6gfZ26A8DVdX9VfVh4GrgXStc93L6beA9wA931u2j/0I3nw8CP0a/43Tb8pW2OIa+FnIj8K/m2lBVXwbWA399RSsas6r6Hv1e3AeS/NCI+xwFPkn/jby53Ar8HLDgBFgT6H8AL+9csbSO/ovYrcBFSdYP1p8F/Djw7SSv7wz1QP/iiEmYOHEsquop+sN67+ms/g3g40n+CkCSs5O8f2i/E8CngJckeetK1Xs6hv7pvTzJoc7jX6x2QWM2PKb/seEGVfVF+n+uz+dGYPOyVbhCqupr9N+g3jlYNTymP9ef9v+Rea6Aq6rngX8PnLssBS+j6t+x+feAf5Dkm8A3gO/Tf5/idcD/SvJ14GvADPAHwCuA2waXLT5C/8Xg+lUofzl9gv5smsALvxu7gK8k2Ud/fP8Hfh4G5/OjzNN5WmnekStJDbGnL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWrI/wcpSIvRu087MAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from matplotlib import pyplot\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "\n",
    "# define undersampling models to test\n",
    "def get_models():\n",
    "\tmodels, names = list(), list()\n",
    "\t# TL\n",
    "\tmodels.append(TomekLinks())\n",
    "\tnames.append('TL')\n",
    "\t# ENN\n",
    "\tmodels.append(EditedNearestNeighbours())\n",
    "\tnames.append('ENN')\n",
    "\t# RENN\n",
    "\tmodels.append(RepeatedEditedNearestNeighbours())\n",
    "\tnames.append('RENN')\n",
    "\t# OSS\n",
    "\tmodels.append(OneSidedSelection())\n",
    "\tnames.append('OSS')\n",
    "\t# NCR\n",
    "\tmodels.append(NeighbourhoodCleaningRule())\n",
    "\tnames.append('NCR')\n",
    "\treturn models, names\n",
    " \n",
    "# define the location of the dataset\n",
    "full_path = 'german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)\n",
    "# define models\n",
    "models, names = get_models()\n",
    "results = list()\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "\t# define model to evaluate\n",
    "\tmodel = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "\t# one hot encode categorical, normalize numerical\n",
    "\tct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "\t# scale, then undersample, then fit model\n",
    "\tpipeline = Pipeline(steps=[('t',ct), ('s', models[i]), ('m',model)])\n",
    "\t# evaluate the model and store results\n",
    "\tscores = evaluate_model(X, y, pipeline)\n",
    "\tresults.append(scores)\n",
    "\t# summarize and store\n",
    "\tprint('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))\n",
    "# plot the results\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that three of the five undersampling techniques resulted in an F2-measure that provides an improvement over the baseline of 0.682. Specifically, ENN, RENN and NCR, with repeated edited nearest neighbors resulting in the best performance with an F2-measure of about 0.716."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Model Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement #1: InstanceHardnessThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.under_sampling import EditedNearestNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.722 (0.038)\n"
     ]
    }
   ],
   "source": [
    "full_path = 'german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)\n",
    "# define model to evaluate\n",
    "model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "# define the data sampling\n",
    "sampling = InstanceHardnessThreshold()\n",
    "# one hot encode categorical, normalize numerical\n",
    "ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "# scale, then sample, then fit model\n",
    "pipeline = Pipeline(steps=[('t',ct), ('s', sampling), ('m',model)])\n",
    "# evaluate the model and store results\n",
    "scores = evaluate_model(X, y, pipeline)\n",
    "print('%.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement #2: SMOTEENN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An F2-measure of about 0.730 can be achieved using LDA with SMOTEENN, where the ENN parameter is set to an ENN instance with sampling_strategy set to majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.727 (0.030)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)\n",
    "# define model to evaluate\n",
    "model = LinearDiscriminantAnalysis()\n",
    "# define the data sampling\n",
    "sampling = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "# one hot encode categorical, normalize numerical\n",
    "ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "# scale, then sample, then fit model\n",
    "pipeline = Pipeline(steps=[('t',ct), ('s', sampling), ('m',model)])\n",
    "# evaluate the model and store results\n",
    "scores = evaluate_model(X, y, pipeline)\n",
    "print('%.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement #3: SMOTEENN with StandardScaler and RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.729 (0.035)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)\n",
    "# define model to evaluate\n",
    "model = RidgeClassifier()\n",
    "# define the data sampling\n",
    "sampling = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "# one hot encode categorical, normalize numerical\n",
    "ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',StandardScaler(),num_ix)])\n",
    "# scale, then sample, then fit model\n",
    "pipeline = Pipeline(steps=[('t',ct), ('s', sampling), ('m',model)])\n",
    "# evaluate the model and store results\n",
    "scores = evaluate_model(X, y, pipeline)\n",
    "print('%.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Prediction on New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the variance in results, a selection of any of the undersampling methods is probably sufficient. In this case, we will select logistic regression with Repeated ENN.\n",
    "\n",
    "This model had an F2-measure of about about 0.716 on our test harness.\n",
    "\n",
    "We will use this as our final model and use it to make predictions on new data.\n",
    "\n",
    "First, we can define the model as a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can define the model as a pipeline.\n",
    "\n",
    "Once defined, we can fit it on the entire training dataset.\n",
    "\n",
    "Once fit, we can use it to make predictions for new data by calling the predict() function. This will return the class label of 0 for “good customer”, or 1 for “bad customer”.\n",
    "\n",
    "Importantly, we must use the ColumnTransformer that was fit on the training dataset in the Pipeline to correctly prepare new data using the same transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Customers:\n",
      ">Predicted=0 (expected 0)\n",
      ">Predicted=0 (expected 0)\n",
      ">Predicted=1 (expected 0)\n",
      "Bad Customers:\n",
      ">Predicted=1 (expected 1)\n",
      ">Predicted=1 (expected 1)\n",
      ">Predicted=1 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "# define the location of the dataset\n",
    "full_path = 'german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)\n",
    "# define model to evaluate\n",
    "model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "# one hot encode categorical, normalize numerical\n",
    "ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "# scale, then undersample, then fit model\n",
    "pipeline = Pipeline(steps=[('t',ct), ('s', RepeatedEditedNearestNeighbours()), ('m',model)])\n",
    "# fit the model\n",
    "pipeline.fit(X, y)\n",
    "# evaluate on some good customers cases (known class 0)\n",
    "print('Good Customers:')\n",
    "data = [['A11', 6, 'A34', 'A43', 1169, 'A65', 'A75', 4, 'A93', 'A101', 4, 'A121', 67, 'A143', 'A152', 2, 'A173', 1, 'A192', 'A201'],\n",
    "    ['A14', 12, 'A34', 'A46', 2096, 'A61', 'A74', 2, 'A93', 'A101', 3, 'A121', 49, 'A143', 'A152', 1, 'A172', 2, 'A191', 'A201'],\n",
    "    ['A11', 42, 'A32', 'A42', 7882, 'A61', 'A74', 2, 'A93', 'A103', 4, 'A122', 45, 'A143', 'A153', 1, 'A173', 2, 'A191', 'A201']]\n",
    "for row in data:\n",
    "    # make prediction\n",
    "    yhat = pipeline.predict([row])\n",
    "    # get the label\n",
    "    label = yhat[0]\n",
    "    # summarize\n",
    "    print('>Predicted=%d (expected 0)' % (label))\n",
    "# evaluate on some bad customers (known class 1)\n",
    "print('Bad Customers:')\n",
    "data = [['A13', 18, 'A32', 'A43', 2100, 'A61', 'A73', 4, 'A93', 'A102', 2, 'A121', 37, 'A142', 'A152', 1, 'A173', 1, 'A191', 'A201'],\n",
    "    ['A11', 24, 'A33', 'A40', 4870, 'A61', 'A73', 3, 'A93', 'A101', 4, 'A124', 53, 'A143', 'A153', 2, 'A173', 2, 'A191', 'A201'],\n",
    "    ['A11', 24, 'A32', 'A43', 1282, 'A62', 'A73', 4, 'A92', 'A101', 2, 'A123', 32, 'A143', 'A152', 1, 'A172', 1, 'A191', 'A201']]\n",
    "for row in data:\n",
    "    # make prediction\n",
    "    yhat = pipeline.predict([row])\n",
    "    # get the label\n",
    "    label = yhat[0]\n",
    "    # summarize\n",
    "    print('>Predicted=%d (expected 1)' % (label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
